{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Notes: In this lesson, we'll take a look at spaCy's matcher, which lets you write rules to find words and phrases in text."
      ],
      "metadata": {
        "id": "nALEMtHUcoFi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFe7GPo3ZdvB",
        "outputId": "21875c65-f443-441d-a009-202b8c7b6cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-16 12:13:16--  https://www.gutenberg.org/files/11/11-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 174313 (170K) [text/plain]\n",
            "Saving to: ‘11-0.txt’\n",
            "\n",
            "11-0.txt            100%[===================>] 170.23K   705KB/s    in 0.2s    \n",
            "\n",
            "2023-10-16 12:13:16 (705 KB/s) - ‘11-0.txt’ saved [174313/174313]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.gutenberg.org/files/11/11-0.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, here's the table of token attributes in text processing using spaCy:\n",
        "\n",
        "| ATTRIBUTE       | VALUE TYPE | DESCRIPTION                                      |\n",
        "|-----------------|------------|--------------------------------------------------|\n",
        "| ORTH            | unicode    | The exact verbatim text of a token.             |\n",
        "| TEXT V2.1       | unicode    | The exact verbatim text of a token.             |\n",
        "| LOWER           | unicode    | The lowercase form of the token text.           |\n",
        "| LENGTH          | int        | The length of the token text.                   |\n",
        "| IS_ALPHA        | bool       | Token text consists of alphabetic characters.   |\n",
        "| IS_ASCII        | bool       | Token text consists of ASCII characters.       |\n",
        "| IS_DIGIT        | bool       | Token text consists of digits.                 |\n",
        "| IS_LOWER        | bool       | Token text is in lowercase.                    |\n",
        "| IS_UPPER        | bool       | Token text is in uppercase.                    |\n",
        "| IS_TITLE        | bool       | Token text is in titlecase.                    |\n",
        "| IS_PUNCT        | bool       | Token is punctuation.                           |\n",
        "| IS_SPACE        | bool       | Token is whitespace.                            |\n",
        "| IS_STOP         | bool       | Token is a stop word.                          |\n",
        "| IS_SENT_START   | bool       | Token is the start of a sentence.              |\n",
        "| SPACY           | bool       | Token has a trailing space.                    |\n",
        "| LIKE_NUM        | bool       | Token text resembles a number.                |\n",
        "| LIKE_URL        | bool       | Token text resembles a URL.                    |\n",
        "| LIKE_EMAIL      | bool       | Token text resembles an email address.        |\n",
        "| POS             | unicode    | The token's simple part-of-speech tag.         |\n",
        "| TAG             | unicode    | The token's part-of-speech tag.                |\n",
        "| DEP             | unicode    | The token's dependency label.                  |\n",
        "| LEMMA           | unicode    | The lemma (base form) of the token.            |\n",
        "| SHAPE           | unicode    | The visual shape of the token.                 |\n",
        "| ENT_TYPE        | unicode    | The entity label of the token.                |"
      ],
      "metadata": {
        "id": "vcDYss2Hfupw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model in spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "text = \"The quick brown fox jumped over the lazy dog.\"\n",
        "docs = nlp(text)\n",
        "\n",
        "# Iterate through the tokens and print their attributes\n",
        "for token in docs:\n",
        "    print(f\"Token: {token.text}\")\n",
        "    print(f\"POS Tag: {token.pos_}\")    # Part-of-Speech (POS) Tag\n",
        "    print(f\"Tag: {token.tag_}\")        # Detailed POS Tag\n",
        "    print(f\"Dependency Label: {token.dep_}\")\n",
        "    print(f\"Lemma: {token.lemma_}\")\n",
        "    print(f\"Shape: {token.shape_}\")\n",
        "    print(f\"Entity Type: {token.ent_type_}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOLcbeN4gQ4T",
        "outputId": "f8fa4f01-0682-4542-90b6-fe37abdeaa25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: The\n",
            "POS Tag: DET\n",
            "Tag: DT\n",
            "Dependency Label: det\n",
            "Lemma: the\n",
            "Shape: Xxx\n",
            "Entity Type: \n",
            "\n",
            "Token: quick\n",
            "POS Tag: ADJ\n",
            "Tag: JJ\n",
            "Dependency Label: amod\n",
            "Lemma: quick\n",
            "Shape: xxxx\n",
            "Entity Type: \n",
            "\n",
            "Token: brown\n",
            "POS Tag: ADJ\n",
            "Tag: JJ\n",
            "Dependency Label: amod\n",
            "Lemma: brown\n",
            "Shape: xxxx\n",
            "Entity Type: \n",
            "\n",
            "Token: fox\n",
            "POS Tag: NOUN\n",
            "Tag: NN\n",
            "Dependency Label: nsubj\n",
            "Lemma: fox\n",
            "Shape: xxx\n",
            "Entity Type: \n",
            "\n",
            "Token: jumped\n",
            "POS Tag: VERB\n",
            "Tag: VBD\n",
            "Dependency Label: ROOT\n",
            "Lemma: jump\n",
            "Shape: xxxx\n",
            "Entity Type: \n",
            "\n",
            "Token: over\n",
            "POS Tag: ADP\n",
            "Tag: IN\n",
            "Dependency Label: prep\n",
            "Lemma: over\n",
            "Shape: xxxx\n",
            "Entity Type: \n",
            "\n",
            "Token: the\n",
            "POS Tag: DET\n",
            "Tag: DT\n",
            "Dependency Label: det\n",
            "Lemma: the\n",
            "Shape: xxx\n",
            "Entity Type: \n",
            "\n",
            "Token: lazy\n",
            "POS Tag: ADJ\n",
            "Tag: JJ\n",
            "Dependency Label: amod\n",
            "Lemma: lazy\n",
            "Shape: xxxx\n",
            "Entity Type: \n",
            "\n",
            "Token: dog\n",
            "POS Tag: NOUN\n",
            "Tag: NN\n",
            "Dependency Label: pobj\n",
            "Lemma: dog\n",
            "Shape: xxx\n",
            "Entity Type: \n",
            "\n",
            "Token: .\n",
            "POS Tag: PUNCT\n",
            "Tag: .\n",
            "Dependency Label: punct\n",
            "Lemma: .\n",
            "Shape: .\n",
            "Entity Type: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set varible\n",
        "PATH = \"/content/11-0.txt\"\n",
        "\n",
        "#reading the data\n",
        "data = open(PATH).read()\n",
        "\n",
        "#if you get an error try the following\n",
        "#data = open('11-0.txt',encoding = 'cp850').read()\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Import the Matcher\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(data)"
      ],
      "metadata": {
        "id": "bH2Qn-F5Z_RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s say we want to find phrases starting with the word Alice followed by a verb."
      ],
      "metadata": {
        "id": "Q4qBaJRZbpiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Create a pattern matching two tokens: \"Alice\" and a Verb\n",
        "#TEXT is for the exact match and VERB for a verb\n",
        "pattern = [{\"TEXT\": \"Alice\"}, {\"POS\": \"VERB\"}]\n",
        "\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "\n",
        "#the first variable is a unique id for the pattern (alice).\n",
        "#The second is an optional callback and the third one is our pattern.\n",
        "matcher.add(\"alice\", [pattern])\n",
        "\n",
        "# Use the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-s6yS46bIrW",
        "outputId": "ddc752b0-9dd3-4bb9-87c4-836b30c9f3ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches: ['Alice think', 'Alice started', 'Alice had', 'Alice had', 'Alice began', 'Alice opened', 'Alice ventured', 'Alice felt', 'Alice took', 'Alice thought', 'Alice had', 'Alice went', 'Alice went', 'Alice thought', 'Alice kept', 'Alice had', 'Alice thought', 'Alice called', 'Alice replied', 'Alice began', 'Alice guessed', 'Alice said', 'Alice went', 'Alice knew', 'Alice heard', 'Alice thought', 'Alice heard', 'Alice noticed', 'Alice dodged', 'Alice looked', 'Alice looked', 'Alice replied', 'Alice replied', 'Alice felt', 'Alice turned', 'Alice thought', 'Alice replied', 'Alice folded', 'Alice said', 'Alice waited', 'Alice crouched', 'Alice noticed', 'Alice laughed', 'Alice went', 'Alice thought', 'Alice said', 'Alice said', 'Alice glanced', 'Alice caught', 'Alice looked', 'Alice added', 'Alice felt', 'Alice remarked', 'Alice waited', 'Alice coming', 'Alice looked', 'Alice said', 'Alice thought', 'Alice considered', 'Alice replied', 'Alice felt', 'Alice replied', 'Alice sighed', 'Alice asked', 'Alice ventured', 'Alice tried', 'Alice replied', 'Alice said', 'Alice said', 'Alice thought', 'Alice looked', 'Alice recognised', 'Alice joined', 'Alice gave', 'Alice thought', 'Alice found', 'Alice began', 'Alice waited', 'Alice put', 'Alice began', 'Alice thought', 'Alice appeared', 'Alice ventured', 'Alice whispered', 'Alice thought', 'Alice remarked', 'Alice said', 'Alice said', 'Alice looked', 'Alice heard', 'Alice thought', 'Alice asked', 'Alice ventured', 'Alice went', 'Alice began', 'Alice replied', 'Alice looked', 'Alice asked', 'Alice began', 'Alice said', 'Alice said', 'Alice panted', 'Alice whispered', 'Alice began', 'Alice felt', 'Alice guessed', 'Alice watched', 'Alice looked', 'Alice got']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find adjectives followed by a noun ."
      ],
      "metadata": {
        "id": "EolccuQTecWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
        "matcher.add(\"id1\", [pattern])\n",
        "matches = matcher(doc)\n",
        "# We will show you the first 20 matches\n",
        "print(\"Matches:\", set([doc[start:end].text for match_id, start, end in matches][:20]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzvmNtCSbISV",
        "outputId": "37c2e25a-d7bb-4e39-d0e5-6b4a07d447ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches: {'large rabbit', 'golden key', 'dreamy sort', 'long passage', 'other parts', 'little girl', 'grand words', 'right distance', 'dry leaves', '* START', 'legged table', 'hot day', 'low hall', 'first thought', 'good opportunity', 'many miles', 'pink eyes', 'several things', 'own mind', 'right word'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Match begin as LEMMA followed by an adposition"
      ],
      "metadata": {
        "id": "OFHX5PCYeweA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"LEMMA\": \"begin\"},{\"POS\": \"ADP\"}]\n",
        "matcher.add(\"id1\", [pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", set([doc[start:end].text for match_id, start, end in matches]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S60eB3fqZ-1P",
        "outputId": "e0c975a9-6e86-46b6-dc36-68b2eab3e375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches: {'began in', 'begins with', 'beginning from', 'begin at', 'began by', 'begin with', 'beginning with', 'Begin at'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantifier\n",
        "\n",
        "\n",
        "| OP  | DESCRIPTION                                                    |\n",
        "|-----|----------------------------------------------------------------|\n",
        "| !   | Negate the pattern, requiring it to match exactly 0 times.     |\n",
        "| ?   | Make the pattern optional, allowing it to match 0 or 1 times.   |\n",
        "| +   | Require the pattern to match 1 or more times.                   |\n",
        "| *   | Allow the pattern to match zero or more times.                 |"
      ],
      "metadata": {
        "id": "qG5spcSigrtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For example, match the exact word Alice followed by zero or more punctuations:\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"TEXT\": \"Alice\"}, {\"IS_PUNCT\": True,\"OP\":\"*\"}]\n",
        "matcher.add(\"id1\", [pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", set([doc[start:end].text for match_id, start, end in matches]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyCpCKkXguix",
        "outputId": "ab8e315d-b784-42d8-8569-40db7a8cd75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches: {'Alice.', 'Alice, “', 'Alice,) “', 'Alice: “', 'Alice: “—', 'Alice,)', 'Alice:', 'Alice, (', 'Alice!”', 'Alice; “', 'Alice', 'Alice (', 'Alice,', 'Alice;', 'Alice!', 'Alice. “'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REGEX\n",
        "\n",
        "Example: Match all words starting with “a” followed by parts of speech that start with “V” (VERB etc)"
      ],
      "metadata": {
        "id": "CL2uf8P4hWYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"TEXT\": {\"REGEX\": \"^a\"}},{\"POS\": {\"REGEX\": \"^V\"}}]\n",
        "matcher.add(\"country\", [pattern])\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", set([doc[start:end].text for match_id, start, end in matches][:20]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGCManoUgvk1",
        "outputId": "f852ce7a-6623-4e9e-9f9f-5e5b49034524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches: {'and round', 'are located', 'all made', 'away went', 'and burning', 'about stopping', 'all round', 'and saying', 'and see', 'all think', 'and finding', 'and looked', 'and noticed', 'and went', 'and found', 'all locked', 'and make', 'and wander', 'all seemed'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add and Remove Patterns\n",
        "\n",
        "You can add more patterns to the Macther before running it. You onlly need to use unique ids for every pattern.\n"
      ],
      "metadata": {
        "id": "IEu6ABqMhyOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "pattern = [{\"TEXT\": \"Alice\"}, {\"IS_PUNCT\": True,\"OP\":\"*\"}]\n",
        "matcher.add(\"id1\", [pattern])\n",
        "\n",
        "pattern = [{\"POS\": \"ADJ\"},{\"LOWER\":\"rabbit\"}]\n",
        "matcher.add(\"id2\", [pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", set([doc[start:end].text for match_id, start, end in matches]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6tqqKDdgvdh",
        "outputId": "cb1dc21f-2208-4d50-c397-4ab31f1a20ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches: {'large rabbit', 'Alice.', 'Alice, “', 'Alice,) “', 'Alice: “', 'Alice: “—', 'Alice,)', 'Alice:', 'Alice, (', 'Alice!”', 'Alice; “', 'Alice', 'Alice (', 'Alice,', 'Alice;', 'Alice!', 'Alice. “'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matcher.remove('id1')\n",
        "\n",
        "matches = matcher(doc)\n",
        "print(\"Matches:\", set([doc[start:end].text for match_id, start, end in matches]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkfledfIgvS8",
        "outputId": "50395f8d-7524-42bd-80ec-b2bd7daf7a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches: {'large rabbit'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "refrensi:\n",
        "- https://pythonwife.com/rule-based-matching-with-spacy/"
      ],
      "metadata": {
        "id": "a3U4WaM2kdfk"
      }
    }
  ]
}